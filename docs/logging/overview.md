# –°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

## –û–±–∑–æ—Ä

–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ Dubai.

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è    ‚îÇ    ‚îÇ   Filebeat      ‚îÇ    ‚îÇ   Logstash      ‚îÇ
‚îÇ   (–õ–æ–≥–∏)        ‚îÇ    ‚îÇ   (–°–±–æ—Ä)        ‚îÇ    ‚îÇ   (–û–±—Ä–∞–±–æ—Ç–∫–∞)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Elasticsearch  ‚îÇ
                    ‚îÇ   (–•—Ä–∞–Ω–µ–Ω–∏–µ)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Kibana      ‚îÇ
                    ‚îÇ  (–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ª–æ–≥–æ–≤
- **Real Estate API** - Django –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
- **Analytics Service** - FastAPI —Å–µ—Ä–≤–∏—Å
- **AI Gateway** - AI —Å–µ—Ä–≤–∏—Å—ã
- **Memory LLM Service** - Java —Å–µ—Ä–≤–∏—Å
- **PostgreSQL** - –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
- **Redis** - –ö—ç—à –∏ –æ—á–µ—Ä–µ–¥–∏
- **Nginx** - Reverse proxy
- **Docker** - –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã

## üìù –¢–∏–ø—ã –ª–æ–≥–æ–≤

### Application Logs
```python
import logging
import json
from datetime import datetime

## –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
class StructuredLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # JSON —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä
        formatter = logging.Formatter(
            '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
        )
        
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def log_request(self, method, endpoint, status, duration, user_id=None):
        log_data = {
            "event_type": "http_request",
            "method": method,
            "endpoint": endpoint,
            "status": status,
            "duration_ms": duration,
            "user_id": user_id
        }
        self.logger.info(json.dumps(log_data))
    
    def log_error(self, error, context=None):
        log_data = {
            "event_type": "error",
            "error_type": type(error).__name__,
            "error_message": str(error),
            "context": context
        }
        self.logger.error(json.dumps(log_data))

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger = StructuredLogger('dubai_api')
logger.log_request('GET', '/api/properties', 200, 150, user_id=123)
```

### Business Logs
```python
## –õ–æ–≥–∏ –±–∏–∑–Ω–µ—Å-—Å–æ–±—ã—Ç–∏–π
class BusinessLogger:
    def __init__(self):
        self.logger = logging.getLogger('business')
    
    def log_property_created(self, property_id, user_id, price, location):
        log_data = {
            "event_type": "property_created",
            "property_id": property_id,
            "user_id": user_id,
            "price": price,
            "location": location,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.logger.info(json.dumps(log_data))
    
    def log_user_login(self, user_id, ip_address, user_agent):
        log_data = {
            "event_type": "user_login",
            "user_id": user_id,
            "ip_address": ip_address,
            "user_agent": user_agent,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.logger.info(json.dumps(log_data))
    
    def log_ai_query(self, user_id, query, agent_type, response_time):
        log_data = {
            "event_type": "ai_query",
            "user_id": user_id,
            "query": query,
            "agent_type": agent_type,
            "response_time_ms": response_time,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.logger.info(json.dumps(log_data))

business_logger = BusinessLogger()
```

### System Logs
```python
## –°–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏
class SystemLogger:
    def __init__(self):
        self.logger = logging.getLogger('system')
    
    def log_database_connection(self, status, connection_count):
        log_data = {
            "event_type": "database_connection",
            "status": status,
            "connection_count": connection_count,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.logger.info(json.dumps(log_data))
    
    def log_memory_usage(self, memory_mb, threshold_mb):
        log_data = {
            "event_type": "memory_usage",
            "memory_mb": memory_mb,
            "threshold_mb": threshold_mb,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.logger.info(json.dumps(log_data))
    
    def log_service_health(self, service_name, status, response_time):
        log_data = {
            "event_type": "service_health",
            "service_name": service_name,
            "status": status,
            "response_time_ms": response_time,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.logger.info(json.dumps(log_data))

system_logger = SystemLogger()
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

### Django –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
```python
## settings.py
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'json': {
            'format': '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'verbose': {
            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',
            'style': '{',
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'json',
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'logs/dubai_api.log',
            'maxBytes': 10*1024*1024,  # 10MB
            'backupCount': 5,
            'formatter': 'json',
        },
        'elastic': {
            'class': 'logging.handlers.HTTPHandler',
            'host': 'localhost:9200',
            'url': '/dubai-logs/_doc',
            'method': 'POST',
            'formatter': 'json',
        },
    },
    'loggers': {
        'dubai_api': {
            'handlers': ['console', 'file', 'elastic'],
            'level': 'INFO',
            'propagate': False,
        },
        'dubai_auth': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False,
        },
        'dubai_property': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False,
        },
        'dubai_ai': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False,
        },
    },
}
```

### FastAPI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
```python
## main.py
import logging
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import time
import json

app = FastAPI()

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s"}'
)
logger = logging.getLogger(__name__)

## Middleware –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥—è—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    logger.info(json.dumps({
        "event_type": "request_start",
        "method": request.method,
        "url": str(request.url),
        "client_ip": request.client.host,
        "user_agent": request.headers.get("user-agent")
    }))
    
    response = await call_next(request)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
    duration = time.time() - start_time
    logger.info(json.dumps({
        "event_type": "request_end",
        "method": request.method,
        "url": str(request.url),
        "status_code": response.status_code,
        "duration_ms": round(duration * 1000, 2)
    }))
    
    return response

## –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(json.dumps({
        "event_type": "exception",
        "method": request.method,
        "url": str(request.url),
        "error_type": type(exc).__name__,
        "error_message": str(exc)
    }))
    raise exc
```

### Java Spring Boot –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
```xml
<!-- logback-spring.xml -->
<configuration>
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp/>
                <logLevel/>
                <loggerName/>
                <message/>
                <mdc/>
                <stackTrace/>
            </providers>
        </encoder>
    </appender>
    
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/dubai_memory_llm.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/dubai_memory_llm.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp/>
                <logLevel/>
                <loggerName/>
                <message/>
                <mdc/>
                <stackTrace/>
            </providers>
        </encoder>
    </appender>
    
    <logger name="com.dubai.memoryllm" level="INFO"/>
    
    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="FILE"/>
    </root>
</configuration>
```

## üìä –†–æ—Ç–∞—Ü–∏—è –∏ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–æ—Ç–∞—Ü–∏–∏ –ª–æ–≥–æ–≤
```python
import logging
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
import gzip
import os

## –†–æ—Ç–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É
def setup_size_rotation_logger(name, log_file, max_bytes=10*1024*1024, backup_count=5):
    logger = logging.getLogger(name)
    handler = RotatingFileHandler(
        log_file,
        maxBytes=max_bytes,
        backupCount=backup_count
    )
    
    formatter = logging.Formatter(
        '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
    )
    handler.setFormatter(formatter)
    
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    return logger

## –†–æ—Ç–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
def setup_time_rotation_logger(name, log_file, when='midnight', interval=1, backup_count=30):
    logger = logging.getLogger(name)
    handler = TimedRotatingFileHandler(
        log_file,
        when=when,
        interval=interval,
        backupCount=backup_count
    )
    
    formatter = logging.Formatter(
        '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
    )
    handler.setFormatter(formatter)
    
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    return logger

## –°–∂–∞—Ç–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
def compress_old_logs(log_directory):
    for filename in os.listdir(log_directory):
        if filename.endswith('.log') and not filename.endswith('.gz'):
            filepath = os.path.join(log_directory, filename)
            with open(filepath, 'rb') as f_in:
                with gzip.open(filepath + '.gz', 'wb') as f_out:
                    f_out.writelines(f_in)
            os.remove(filepath)

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
api_logger = setup_size_rotation_logger('dubai_api', 'logs/api.log')
system_logger = setup_time_rotation_logger('dubai_system', 'logs/system.log')
```

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
```python
import os
import time
from datetime import datetime, timedelta

def cleanup_old_logs(log_directory, days_to_keep=30):
    """–£–¥–∞–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ —Å—Ç–∞—Ä—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π"""
    cutoff_time = time.time() - (days_to_keep * 24 * 60 * 60)
    
    for filename in os.listdir(log_directory):
        filepath = os.path.join(log_directory, filename)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        if os.path.isfile(filepath):
            if os.path.getmtime(filepath) < cutoff_time:
                try:
                    os.remove(filepath)
                    print(f"–£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {filename}")
                except OSError as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {filename}: {e}")

def cleanup_by_size(log_directory, max_size_mb=1000):
    """–û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤ –ø–æ –æ–±—â–µ–º—É —Ä–∞–∑–º–µ—Ä—É"""
    total_size = 0
    files = []
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–∞—Ö
    for filename in os.listdir(log_directory):
        filepath = os.path.join(log_directory, filename)
        if os.path.isfile(filepath):
            size = os.path.getsize(filepath)
            files.append((filepath, size, os.path.getmtime(filepath)))
            total_size += size
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è (—Å—Ç–∞—Ä—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    files.sort(key=lambda x: x[2])
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    max_size_bytes = max_size_mb * 1024 * 1024
    for filepath, size, _ in files:
        if total_size <= max_size_bytes:
            break
        
        try:
            os.remove(filepath)
            total_size -= size
            print(f"–£–¥–∞–ª–µ–Ω –ª–æ–≥ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –º–µ—Å—Ç–∞: {os.path.basename(filepath)}")
        except OSError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {filepath}: {e}")

## –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ—á–∏—Å—Ç–∫–∏
import schedule

def schedule_log_cleanup():
    # –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤ 2:00
    schedule.every().day.at("02:00").do(cleanup_old_logs, 'logs', 30)
    
    # –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É
    schedule.every().sunday.at("03:00").do(cleanup_by_size, 'logs', 1000)
    
    while True:
        schedule.run_pending()
        time.sleep(60)

## –ó–∞–ø—É—Å–∫ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
import threading
cleanup_thread = threading.Thread(target=schedule_log_cleanup, daemon=True)
cleanup_thread.start()
```

## üîç –ü–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤

### ELK Stack –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
```yaml
## docker-compose.logging.yml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.0
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline
      - ./logs:/logs
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.0
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.17.0
    volumes:
      - ./monitoring/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/logs:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro

volumes:
  elasticsearch_data:
```

### Filebeat –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
```yaml
## filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /logs/*.log
  json.keys_under_root: true
  json.add_error_key: true
  json.message_key: message

- type: container
  paths:
    - '/var/lib/docker/containers/*/*.log'

processors:
- add_host_metadata: ~
- add_cloud_metadata: ~
- add_docker_metadata: ~
- add_kubernetes_metadata: ~

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  indices:
    - index: "dubai-logs-%{+yyyy.MM.dd}"

setup.kibana:
  host: "kibana:5601"

setup.dashboards.enabled: true
setup.template.enabled: true
```

### Logstash pipeline
```ruby
## pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "dubai_api" {
    json {
      source => "message"
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    if [event_type] == "http_request" {
      mutate {
        add_field => { "log_type" => "api_request" }
      }
    }
    
    if [event_type] == "error" {
      mutate {
        add_field => { "log_type" => "error" }
      }
    }
  }
  
  if [fields][service] == "dubai_ai" {
    json {
      source => "message"
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_field => { "log_type" => "ai_service" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "dubai-logs-%{+YYYY.MM.dd}"
  }
}
```

## üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Kibana

### –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥—ã

#### 1. API Requests Dashboard
```json
{
  "dashboard": {
    "title": "API Requests Overview",
    "panels": [
      {
        "title": "Requests per Second",
        "type": "visualization",
        "visState": {
          "type": "line",
          "aggs": [
            {
              "id": "1",
              "enabled": true,
              "type": "count",
              "schema": "metric",
              "params": {}
            },
            {
              "id": "2",
              "enabled": true,
              "type": "date_histogram",
              "schema": "segment",
              "params": {
                "field": "timestamp",
                "timeRange": {
                  "from": "now-1h",
                  "to": "now"
                },
                "useNormalizedEsInterval": true,
                "scaleMetricValues": false,
                "interval": "auto",
                "drop_partials": false,
                "min_doc_count": 1,
                "extended_bounds": {}
              }
            }
          ]
        }
      }
    ]
  }
}
```

#### 2. Error Analysis Dashboard
```json
{
  "dashboard": {
    "title": "Error Analysis",
    "panels": [
      {
        "title": "Error Count by Type",
        "type": "visualization",
        "visState": {
          "type": "pie",
          "aggs": [
            {
              "id": "1",
              "enabled": true,
              "type": "count",
              "schema": "metric",
              "params": {}
            },
            {
              "id": "2",
              "enabled": true,
              "type": "terms",
              "schema": "segment",
              "params": {
                "field": "error_type",
                "size": 10,
                "order": "desc",
                "orderBy": "1"
              }
            }
          ]
        }
      }
    ]
  }
}
```

## üöÄ –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

### –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
```bash
## –ó–∞–ø—É—Å–∫ ELK Stack
docker-compose -f docker-compose.logging.yml up -d

## –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
docker-compose -f docker-compose.logging.yml ps

## –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
docker-compose -f docker-compose.logging.yml logs -f elasticsearch
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
```bash
## Elasticsearch
curl http://localhost:9200/_cluster/health

## Kibana
open http://localhost:5601

## –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤
curl http://localhost:9200/_cat/indices?v
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [ELK Stack Documentation](https://www.elastic.co/guide/)
- [Filebeat Documentation](https://www.elastic.co/guide/en/beats/filebeat/current/index.html)
- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)

### –ü–æ–ª–µ–∑–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- [Elasticsearch Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html)
- [Kibana Query Language](https://www.elastic.co/guide/en/kibana/current/kuery-query.html)

---

**–°—Ç–∞—Ç—É—Å**: –ê–∫—Ç–∏–≤–Ω—ã–π  
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ**: –ê–≤–≥—É—Å—Ç 2025

